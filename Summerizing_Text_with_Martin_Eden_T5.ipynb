{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIL5nZ0W7S1U",
        "outputId": "574e0e22-4b24-4e2b-88cb-ffb698683d61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece==0.1.94"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmGlo1msveMU",
        "outputId": "8e12836e-4e36-4540-f54a-1485b2debf8d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece==0.1.94 in /usr/local/lib/python3.10/dist-packages (0.1.94)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_architecture=True"
      ],
      "metadata": {
        "id": "Ye5BkkzYv2ck"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
        "device = torch.device('cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNvVXqkAv6H_",
        "outputId": "4bfe9429-f097-46cc-ad71-e840ecfc2827"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj1EOl5WwNXJ",
        "outputId": "a0ed8874-142f-42c3-e519-5f61f4eaeb9e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if display_architecture==True:\n",
        " print(model.config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7Cp_AfowlTR",
        "outputId": "bf3f7cd1-6882-404a-c3fa-174d830b7dd5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5Config {\n",
            "  \"_name_or_path\": \"t5-large\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 4096,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"relu\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": false,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 24,\n",
            "  \"num_heads\": 16,\n",
            "  \"num_layers\": 24,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.35.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if(display_architecture==True):\n",
        "  print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsr9_yLFwPrl",
        "outputId": "d8371f5a-a6d3-4652-d317-8d32cab26117"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5ForConditionalGeneration(\n",
            "  (shared): Embedding(32128, 1024)\n",
            "  (encoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32128, 1024)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 16)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-23): 23 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32128, 1024)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 16)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-23): 23 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if display_architecture==True:\n",
        "  print(model.encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHYRzd5Sws-a",
        "outputId": "2fbc1ab3-6966-45c2-ac8b-52deb213edcb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5Stack(\n",
            "  (embed_tokens): Embedding(32128, 1024)\n",
            "  (block): ModuleList(\n",
            "    (0): T5Block(\n",
            "      (layer): ModuleList(\n",
            "        (0): T5LayerSelfAttention(\n",
            "          (SelfAttention): T5Attention(\n",
            "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (relative_attention_bias): Embedding(32, 16)\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (1): T5LayerFF(\n",
            "          (DenseReluDense): T5DenseActDense(\n",
            "            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (act): ReLU()\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1-23): 23 x T5Block(\n",
            "      (layer): ModuleList(\n",
            "        (0): T5LayerSelfAttention(\n",
            "          (SelfAttention): T5Attention(\n",
            "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (1): T5LayerFF(\n",
            "          (DenseReluDense): T5DenseActDense(\n",
            "            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (act): ReLU()\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (final_layer_norm): T5LayerNorm()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if display_architecture==True:\n",
        "  print(model.decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQmT7rS7w4bK",
        "outputId": "f757e22b-695a-4454-ce5c-9b6e326876ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5Stack(\n",
            "  (embed_tokens): Embedding(32128, 1024)\n",
            "  (block): ModuleList(\n",
            "    (0): T5Block(\n",
            "      (layer): ModuleList(\n",
            "        (0): T5LayerSelfAttention(\n",
            "          (SelfAttention): T5Attention(\n",
            "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (relative_attention_bias): Embedding(32, 16)\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (1): T5LayerCrossAttention(\n",
            "          (EncDecAttention): T5Attention(\n",
            "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (2): T5LayerFF(\n",
            "          (DenseReluDense): T5DenseActDense(\n",
            "            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (act): ReLU()\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1-23): 23 x T5Block(\n",
            "      (layer): ModuleList(\n",
            "        (0): T5LayerSelfAttention(\n",
            "          (SelfAttention): T5Attention(\n",
            "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (1): T5LayerCrossAttention(\n",
            "          (EncDecAttention): T5Attention(\n",
            "            (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (2): T5LayerFF(\n",
            "          (DenseReluDense): T5DenseActDense(\n",
            "            (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "            (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (act): ReLU()\n",
            "          )\n",
            "          (layer_norm): T5LayerNorm()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (final_layer_norm): T5LayerNorm()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if display_architecture==True:\n",
        "  print(model.forward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIRJyTWXw-1U",
        "outputId": "1542d2cb-4f98-499f-cb8b-aeee6ae65c79"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method T5ForConditionalGeneration.forward of T5ForConditionalGeneration(\n",
            "  (shared): Embedding(32128, 1024)\n",
            "  (encoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32128, 1024)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 16)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-23): 23 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): T5Stack(\n",
            "    (embed_tokens): Embedding(32128, 1024)\n",
            "    (block): ModuleList(\n",
            "      (0): T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (relative_attention_bias): Embedding(32, 16)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1-23): 23 x T5Block(\n",
            "        (layer): ModuleList(\n",
            "          (0): T5LayerSelfAttention(\n",
            "            (SelfAttention): T5Attention(\n",
            "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (1): T5LayerCrossAttention(\n",
            "            (EncDecAttention): T5Attention(\n",
            "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (2): T5LayerFF(\n",
            "            (DenseReluDense): T5DenseActDense(\n",
            "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
            "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (layer_norm): T5LayerNorm()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (final_layer_norm): T5LayerNorm()\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
            ")>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(text,ml):\n",
        "  preprocess_text = text.strip().replace(\"\\n\",\"\")\n",
        "  t5_prepared_Text = \"summarize: \"+preprocess_text\n",
        "  print (\"Preprocessed and prepared text: \\n\", t5_prepared_Text)\n",
        "\n",
        "  tokenized_text = tokenizer.encode(t5_prepared_Text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  # summmarize\n",
        "  summary_ids = model.generate(tokenized_text,\n",
        "                                      num_beams=4,\n",
        "                                      no_repeat_ngram_size=2,\n",
        "                                      min_length=30,\n",
        "                                      max_length=ml,\n",
        "                                      early_stopping=True)\n",
        "\n",
        "  output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "  return output"
      ],
      "metadata": {
        "id": "5XHLKtAIxFkD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://lotofsense.com/literature/martin-eden-meaning-and-analysis-of-the-book-by-jack-london/?ysclid=lpv1nmzlt4291641959"
      ],
      "metadata": {
        "id": "J2Wm9eQr4z7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\"\n",
        "A man named Martin Eden is trying to overcome the existing proletarian origin, intensively, passionately, enthusiastically engaged in his own self-education, wants to understand the basics and depths of different directions, hopes for a worthy position in society and among the literary American elite. Men’s actions show the true state of affairs in society.\n",
        "\n",
        "There is a desire for love for Ruth Morse, who is a wonderful representative of a bourgeois family. Eden for her is an ordinary and rude man, a poorly educated sailor, he has no right to claim the hand and heart of Ruth, who is always and in everything beautiful, ideal, like a mysterious embodiment of reality.\n",
        "\n",
        "The analysis of the events of that time is simple. A certain sailor is a representative of the working class. Throughout the novel, the listener observes the evolutionary period of the character’s development, his inner growth. From a simple worker, Martin turns into a famous writer, paying a double price for the process, which is justified by getting a good position in society, participating in various events. The high price is right. The events are worth it. The man overcame a difficult path, thorny, strewn not only with joyful minutes of meetings and communication with pleasant people. The presence of deprivation, suffering is provided for the realization of knowledge in all its forms. Many truths are revealed to the hero. There comes a full awareness of situations and methods of communication among people. The character loves his beloved girl, cherishes feelings with all his heart, they lead to a positive outcome.\n",
        "\n",
        "Eden was disappointed in society, the foundations as a whole, the device for him was an incorrect network and a chain of events that continued along with the manifestation of injustice, flattery, hypocrisy, deprivation, feigned importance. The masks of educated intellectuals were noticeable, there was a habit, an inability to tell the truth about simple things. The participation of hypocrites is always welcome. A philosophical assessment of the work, phenomena is given, but Martin’s fate is tragic, unchanged. A life filled with stereotypes has led to a terrible end. When expressing the qualities of a person, individuality, a special interest arises. Friendship with Russ Brissenden becomes a support in all situations, love for Ruth is also important. Discord with his beloved, the death of Brissenden’s friend, who committed suicide, undermine Martin’s strength, leading to the death of the hero.\n",
        "\n",
        "Through hard work, a person can reach heights. The manifestation of ardent notes of individualism leads to the denial of the people, the lack of support from like-minded people, close people, various unpleasant twists of fate, the tragic end of creative processes. The hero did not think about public goods, which led to death. Ignoring the difficulties of the surrounding world by an individualist led to terrible and negative consequences. There was a concentration on personal growth, and advanced thinkers, artists, public figures use other methods of personal development. We need a new religion, inflaming the minds and hearts of people, exclusion of indifference, manifestation of consciousness, strength. Willless victims are automatically destroyed by society.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Number of characters:\",len(text))\n",
        "summary=summarize(text,45)\n",
        "print (\"\\n\\nSummarized text: \\n\",summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mgo2rv_gxe44",
        "outputId": "e9aa9305-0931-469d-cb7c-69ccaee3bb9a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of characters: 3285\n",
            "Preprocessed and prepared text: \n",
            " summarize: A man named Martin Eden is trying to overcome the existing proletarian origin, intensively, passionately, enthusiastically engaged in his own self-education, wants to understand the basics and depths of different directions, hopes for a worthy position in society and among the literary American elite. Men’s actions show the true state of affairs in society.There is a desire for love for Ruth Morse, who is a wonderful representative of a bourgeois family. Eden for her is an ordinary and rude man, a poorly educated sailor, he has no right to claim the hand and heart of Ruth, who is always and in everything beautiful, ideal, like a mysterious embodiment of reality.The analysis of the events of that time is simple. A certain sailor is a representative of the working class. Throughout the novel, the listener observes the evolutionary period of the character’s development, his inner growth. From a simple worker, Martin turns into a famous writer, paying a double price for the process, which is justified by getting a good position in society, participating in various events. The high price is right. The events are worth it. The man overcame a difficult path, thorny, strewn not only with joyful minutes of meetings and communication with pleasant people. The presence of deprivation, suffering is provided for the realization of knowledge in all its forms. Many truths are revealed to the hero. There comes a full awareness of situations and methods of communication among people. The character loves his beloved girl, cherishes feelings with all his heart, they lead to a positive outcome.Eden was disappointed in society, the foundations as a whole, the device for him was an incorrect network and a chain of events that continued along with the manifestation of injustice, flattery, hypocrisy, deprivation, feigned importance. The masks of educated intellectuals were noticeable, there was a habit, an inability to tell the truth about simple things. The participation of hypocrites is always welcome. A philosophical assessment of the work, phenomena is given, but Martin’s fate is tragic, unchanged. A life filled with stereotypes has led to a terrible end. When expressing the qualities of a person, individuality, a special interest arises. Friendship with Russ Brissenden becomes a support in all situations, love for Ruth is also important. Discord with his beloved, the death of Brissenden’s friend, who committed suicide, undermine Martin’s strength, leading to the death of the hero.Through hard work, a person can reach heights. The manifestation of ardent notes of individualism leads to the denial of the people, the lack of support from like-minded people, close people, various unpleasant twists of fate, the tragic end of creative processes. The hero did not think about public goods, which led to death. Ignoring the difficulties of the surrounding world by an individualist led to terrible and negative consequences. There was a concentration on personal growth, and advanced thinkers, artists, public figures use other methods of personal development. We need a new religion, inflaming the minds and hearts of people, exclusion of indifference, manifestation of consciousness, strength. Willless victims are automatically destroyed by society.\n",
            "\n",
            "\n",
            "Summarized text: \n",
            " a man named Martin Eden is trying to overcome the existing proletarian origin. he wants to understand the basics and depths of different directions, hopes for position in society, among the literary american elite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary=summarize(text,90)\n",
        "print (\"\\n\\nSummarized text: \\n\",summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOIuIurv5tYr",
        "outputId": "e96a6dfa-1927-4534-8147-be7f2c3e79da"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed and prepared text: \n",
            " summarize: A man named Martin Eden is trying to overcome the existing proletarian origin, intensively, passionately, enthusiastically engaged in his own self-education, wants to understand the basics and depths of different directions, hopes for a worthy position in society and among the literary American elite. Men’s actions show the true state of affairs in society.There is a desire for love for Ruth Morse, who is a wonderful representative of a bourgeois family. Eden for her is an ordinary and rude man, a poorly educated sailor, he has no right to claim the hand and heart of Ruth, who is always and in everything beautiful, ideal, like a mysterious embodiment of reality.The analysis of the events of that time is simple. A certain sailor is a representative of the working class. Throughout the novel, the listener observes the evolutionary period of the character’s development, his inner growth. From a simple worker, Martin turns into a famous writer, paying a double price for the process, which is justified by getting a good position in society, participating in various events. The high price is right. The events are worth it. The man overcame a difficult path, thorny, strewn not only with joyful minutes of meetings and communication with pleasant people. The presence of deprivation, suffering is provided for the realization of knowledge in all its forms. Many truths are revealed to the hero. There comes a full awareness of situations and methods of communication among people. The character loves his beloved girl, cherishes feelings with all his heart, they lead to a positive outcome.Eden was disappointed in society, the foundations as a whole, the device for him was an incorrect network and a chain of events that continued along with the manifestation of injustice, flattery, hypocrisy, deprivation, feigned importance. The masks of educated intellectuals were noticeable, there was a habit, an inability to tell the truth about simple things. The participation of hypocrites is always welcome. A philosophical assessment of the work, phenomena is given, but Martin’s fate is tragic, unchanged. A life filled with stereotypes has led to a terrible end. When expressing the qualities of a person, individuality, a special interest arises. Friendship with Russ Brissenden becomes a support in all situations, love for Ruth is also important. Discord with his beloved, the death of Brissenden’s friend, who committed suicide, undermine Martin’s strength, leading to the death of the hero.Through hard work, a person can reach heights. The manifestation of ardent notes of individualism leads to the denial of the people, the lack of support from like-minded people, close people, various unpleasant twists of fate, the tragic end of creative processes. The hero did not think about public goods, which led to death. Ignoring the difficulties of the surrounding world by an individualist led to terrible and negative consequences. There was a concentration on personal growth, and advanced thinkers, artists, public figures use other methods of personal development. We need a new religion, inflaming the minds and hearts of people, exclusion of indifference, manifestation of consciousness, strength. Willless victims are automatically destroyed by society.\n",
            "\n",
            "\n",
            "Summarized text: \n",
            " a man named Martin Eden is trying to overcome the existing proletarian origin. he wants to understand the basics and depths of different directions, hopes for position in society, among the literary elite. the hero did not think about public goods, which led to terrible and negative consequences.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}